<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Data Science Interview Questions</title>
  <meta name="description" content="A clean, responsive website of data science interview questions with filters, search, and expandable answers." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #0b1220;
      --card: #121a2b;
      --muted: #7d8aaa;
      --text: #e8ecf6;
      --ring: #7aa2ff;
      --accent: #84d361;
      --accent-2: #58c4c8;
      --danger: #ff6b6b;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
    }
    :root[data-theme="light"] {
      --bg: #f7fafc;
      --card: #ffffff;
      --muted: #5b6876;
      --text: #0b1220;
      --ring: #3b82f6;
      --accent: #16a34a;
      --accent-2: #0ea5e9;
      --danger: #dc2626;
      --shadow: 0 10px 30px rgba(13, 20, 35, .08);
    }
    * { box-sizing: border-box; }
    html, body { height: 100%; }
    body {
      margin: 0;
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
      background: radial-gradient(1200px 800px at 80% -10%, rgba(122,162,255,.25), transparent 60%),
                  radial-gradient(900px 600px at -10% 10%, rgba(88,196,200,.18), transparent 55%),
                  var(--bg);
      color: var(--text);
      line-height: 1.55;
    }
    header {
      position: sticky; top: 0; z-index: 10;
      backdrop-filter: blur(10px);
      background: color-mix(in oklab, var(--bg) 88%, transparent);
      border-bottom: 1px solid color-mix(in oklab, var(--text) 12%, transparent);
    }
    .container { max-width: 1100px; margin: 0 auto; padding: 18px 20px; }
    .brand {
      display: flex; gap: 12px; align-items: center;
      font-weight: 700; letter-spacing: .2px; font-size: 1.05rem;
    }
    .badge { padding: 4px 10px; border-radius: 999px; background: var(--card); color: var(--muted); border: 1px solid color-mix(in oklab, var(--text) 12%, transparent); }
    .toolbar { display: grid; grid-template-columns: 1fr; gap: 10px; }
    .row { display: flex; flex-wrap: wrap; gap: 10px; align-items: center; }
    .search {
      position: relative; flex: 1 1 440px; min-width: 240px;
    }
    .search input {
      width: 100%; padding: 12px 40px 12px 40px; border-radius: 12px;
      background: var(--card); color: var(--text);
      border: 1px solid color-mix(in oklab, var(--text) 14%, transparent);
      box-shadow: var(--shadow);
    }
    .search svg { position: absolute; left: 12px; top: 50%; transform: translateY(-50%); opacity: .7; }
    .search .clear { position: absolute; right: 8px; top: 50%; transform: translateY(-50%); border: none; background: transparent; color: var(--muted); cursor: pointer; font-size: 18px; }
    .select, .chip {
      display: inline-flex; align-items: center; gap: 8px; padding: 10px 12px; border-radius: 12px;
      background: var(--card); color: var(--text);
      border: 1px solid color-mix(in oklab, var(--text) 14%, transparent);
      box-shadow: var(--shadow);
    }
    select, button, summary { font: inherit; }
    select { background: transparent; color: inherit; border: none; outline: none; cursor: pointer; }
    .chip input { accent-color: var(--ring); }
    .btn {
      padding: 10px 14px; border-radius: 12px; border: 1px solid color-mix(in oklab, var(--text) 14%, transparent);
      background: linear-gradient(180deg, color-mix(in oklab, var(--ring) 24%, transparent), transparent);
      color: var(--text); cursor: pointer; box-shadow: var(--shadow);
    }
    .btn.primary { background: linear-gradient(180deg, color-mix(in oklab, var(--accent) 36%, transparent), transparent); }
    .btn.ghost { background: var(--card); }
    .btn.danger { background: linear-gradient(180deg, color-mix(in oklab, var(--danger) 36%, transparent), transparent); }
    .grid { display: grid; grid-template-columns: repeat(1, minmax(0, 1fr)); gap: 14px; margin-top: 14px; }
    @media (min-width: 720px) { .grid { grid-template-columns: repeat(2, minmax(0, 1fr)); } }
    @media (min-width: 1060px) { .grid { grid-template-columns: repeat(3, minmax(0, 1fr)); } }

    .card {
      background: var(--card);
      border: 1px solid color-mix(in oklab, var(--text) 12%, transparent);
      border-radius: 16px; padding: 16px; box-shadow: var(--shadow);
      display: grid; gap: 10px; align-content: start;
    }
    .meta { display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
    .pill { font-size: .8rem; padding: 4px 8px; border-radius: 999px; border: 1px solid color-mix(in oklab, var(--text) 18%, transparent); color: var(--muted); }
    .q { font-weight: 600; font-size: 1rem; }
    details { border-top: 1px dashed color-mix(in oklab, var(--text) 18%, transparent); padding-top: 8px; }
    details[open] summary .caret { transform: rotate(90deg); }
    summary { list-style: none; cursor: pointer; display: flex; align-items: center; gap: 8px; color: var(--muted); }
    summary::-webkit-details-marker { display: none; }
    .actions { display: flex; gap: 8px; }
    .footer { text-align: center; color: var(--muted); padding: 30px 0 60px; }
    .count { color: var(--muted); font-size: .95rem; }
    .kbd { border: 1px solid color-mix(in oklab, var(--text) 18%, transparent); padding: 2px 6px; border-radius: 6px; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size: .85em; color: var(--muted); }
    .bookmark { border-color: color-mix(in oklab, var(--accent) 40%, var(--text) 18%); color: color-mix(in oklab, var(--accent) 60%, var(--text)); }
    .hidden { display: none !important; }
  </style>
</head>
<body>
  <header>
    <div class="container">
      <div class="row" style="justify-content: space-between;">
        <div class="brand" aria-label="Site brand">
          <svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-hidden="true"><path d="M4 7a8 8 0 1 0 16 0" stroke="currentColor" stroke-width="2"/><circle cx="12" cy="7" r="3" stroke="currentColor" stroke-width="2"/></svg>
          <span>Data Science Interview Hub</span>
          <span class="badge" title="Open-source">v1.0</span>
        </div>
        <div class="row">
          <button id="themeBtn" class="btn ghost" title="Toggle theme (L)">Toggle theme <span class="kbd">L</span></button>
          <button id="printBtn" class="btn" title="Print selected (P)">Print <span class="kbd">P</span></button>
          <button id="exportBtn" class="btn" title="Export filtered as JSON">Export</button>
          <button id="resetBtn" class="btn danger" title="Reset filters">Reset</button>
        </div>
      </div>
      <div class="toolbar" style="margin-top: 12px;">
        <div class="row">
          <div class="search" role="search">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" aria-hidden="true"><circle cx="11" cy="11" r="7" stroke="currentColor" stroke-width="2"/><path d="M20 20l-3.5-3.5" stroke="currentColor" stroke-width="2"/></svg>
            <input id="search" placeholder="Search by keyword (regex ok) e.g. ^bias|variance$" aria-label="Search questions" />
            <button class="clear" id="clearSearch" aria-label="Clear search">×</button>
          </div>
          <div class="select" title="Filter by difficulty">
            <span>Difficulty</span>
            <select id="difficulty">
              <option value="all">All</option>
              <option>Easy</option>
              <option>Medium</option>
              <option>Hard</option>
            </select>
          </div>
          <div class="select" title="Sort results">
            <span>Sort</span>
            <select id="sort">
              <option value="default">Default</option>
              <option value="difficulty">Difficulty</option>
              <option value="topic">Topic</option>
              <option value="alpha">A → Z</option>
            </select>
          </div>
          <button id="shuffleBtn" class="btn ghost" title="Shuffle (S)">Shuffle <span class="kbd">S</span></button>
          <div class="chip" title="Show only bookmarks">
            <input type="checkbox" id="onlyBookmarks" /> <label for="onlyBookmarks">Bookmarks</label>
          </div>
        </div>
        <div class="row" id="topicChips" aria-label="Topic filters" style="flex-wrap: wrap"></div>
        <div class="count" id="count"></div>
      </div>
    </div>
  </header>

  <main class="container">
    <section class="grid" id="grid" aria-live="polite"></section>
    <p class="footer">Pro tip: press <span class="kbd">L</span> to toggle theme, <span class="kbd">S</span> to shuffle, and <span class="kbd">/</span> to focus search.</p>
  </main>

  <template id="cardTemplate">
    <article class="card" data-id="">
      <div class="meta">
        <span class="pill topic"></span>
        <span class="pill difficulty"></span>
        <span class="pill time"></span>
      </div>
      <label class="chip" style="margin-left:auto;">
        <input type="checkbox" class="selectPrint" /> Select
      </label>
      <div class="q"></div>
      <details>
        <summary><span class="caret">▶</span> Show answer</summary>
        <div class="a"></div>
      </details>
      <div class="actions">
        <button class="btn ghost copyBtn" aria-label="Copy Q&A">Copy</button>
        <button class="btn ghost bookmarkBtn" aria-label="Bookmark">☆ Save</button>
      </div>
    </article>
  </template>

  <script>
    const el = (sel, root=document) => root.querySelector(sel);
    const els = (sel, root=document) => [...root.querySelectorAll(sel)];

    const QUESTIONS = [
      // --- Statistics & Probability ---
      { id: 'q1', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
        q: 'Explain the difference between population and sample. Why does it matter for inference?',
        a: 'A population contains all units of interest; a sample is a subset used to estimate population parameters. Inference relies on randomness/representativeness: unbiased sampling lets sample statistics (e.g., mean, variance) be consistent estimators of population parameters with quantifiable uncertainty (SE, CIs). Using a sample improperly can introduce sampling bias and invalidate p-values/intervals.' },
      { id: 'q2', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
        q: 'What is the bias–variance tradeoff and how does it influence model selection?',
        a: 'Bias is systematic error from underfitting; variance is sensitivity to training data from overfitting. Total expected error = bias^2 + variance + irreducible noise. Choose models/regularization that minimize validation error—e.g., cross-validation to tune complexity (λ, depth, k) balancing under/overfit.' },
      { id: 'q3', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
        q: 'You flip a fair coin until two heads in a row appear. What is the expected number of flips?',
        a: 'Let E be expectation from scratch; E_H after one head, E_T after tail. Equations: E = 1 + 0.5 E_H + 0.5 E; E_H = 1 + 0.5·0 + 0.5 E; Solve → E = 6. Expected flips = 6.' },
// --- Statistics & Probability ---
{ id: 'q4', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the difference between descriptive and inferential statistics?',
  a: 'Descriptive statistics summarize and describe data (mean, median, variance, plots). Inferential statistics use samples to make generalizations about populations, relying on probability (e.g., hypothesis testing, confidence intervals).' },

{ id: 'q5', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'Define conditional probability and give a real-life example.',
  a: 'Conditional probability is P(A|B) = P(A ∩ B)/P(B). Example: Probability a person has diabetes given obesity. It quantifies likelihood of A given information about B.' },

{ id: 'q6', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the Central Limit Theorem (CLT) and why is it important?',
  a: 'The CLT states that the sampling distribution of the sample mean approaches normality as n→∞, regardless of population distribution (if variance finite). It enables inference via z/t-tests, CIs, etc.' },

{ id: 'q7', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'Two dice are rolled. What is the probability that the sum is divisible by 3?',
  a: 'Possible sums 2–12. Count favorable: sums {3,6,9,12}. Probabilities: (2,1)=2/36; (6)=5/36; (9)=4/36; (12)=1/36. Total = 12/36 = 1/3.' },

{ id: 'q8', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is p-value in hypothesis testing?',
  a: 'The p-value is the probability of observing data as extreme or more extreme than sample, given the null hypothesis is true. Small p-value (< α) suggests evidence against H₀.' },

{ id: 'q9', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is Bayes’ Theorem? Provide an example.',
  a: 'Bayes’ Theorem: P(A|B) = P(B|A)P(A)/P(B). Example: In medical testing, it updates disease probability after positive test using sensitivity, specificity, and prevalence.' },

{ id: 'q10', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the law of large numbers?',
  a: 'As sample size increases, the sample mean converges to the population mean. It justifies why larger samples give more reliable estimates.' },

{ id: 'q11', topic: 'Probability', difficulty: 'Hard', time: '3–4 min',
  q: 'A box contains 3 red, 4 blue, and 5 green balls. Two balls are drawn without replacement. What is the probability they are the same color?',
  a: 'Ways: Red: C(3,2)=3; Blue: C(4,2)=6; Green: C(5,2)=10; total =19. Total draws = C(12,2)=66. Probability = 19/66.' },

{ id: 'q12', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'Distinguish between parametric and nonparametric tests.',
  a: 'Parametric tests assume specific distribution (e.g., t-test assumes normality). Nonparametric tests (Mann-Whitney, Kruskal-Wallis) make fewer assumptions, use ranks, useful for skewed/ordinal data.' },

{ id: 'q13', topic: 'Probability', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the probability of getting exactly 2 heads in 4 fair coin flips?',
  a: 'Number of outcomes = 2^4 =16. Favorable = C(4,2)=6. Probability = 6/16=0.375.' },

{ id: 'q14', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'Explain Type I and Type II errors.',
  a: 'Type I error: reject true H₀ (false positive). Type II error: fail to reject false H₀ (false negative). Significance α controls Type I; power = 1–β controls Type II.' },

{ id: 'q15', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'A fair coin is tossed repeatedly until a head appears. What is the expected number of tosses?',
  a: 'Geometric distribution with p=0.5. Expectation = 1/p = 2 tosses.' },

{ id: 'q16', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is correlation vs causation?',
  a: 'Correlation quantifies linear association between variables. Causation implies one variable directly affects another. Correlation ≠ causation (may be confounding).' },

{ id: 'q17', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the expected value of rolling a fair six-sided die?',
  a: 'E = (1+2+3+4+5+6)/6 = 21/6 = 3.5.' },

{ id: 'q18', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is multicollinearity in regression?',
  a: 'Multicollinearity is when predictors are highly correlated, making coefficient estimates unstable/inflated SEs. Detection: VIF>10. Solution: remove/combine predictors, regularization.' },

{ id: 'q19', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'Find the probability that in a group of 23 people at least two share the same birthday.',
  a: 'Classic birthday paradox: P(no match)= (365×364×...×343)/365^23 ≈ 0.4927. So probability ≥1 match ≈ 1–0.4927=0.5073 (~50.7%).' },

{ id: 'q20', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is standard deviation and what does it measure?',
  a: 'Standard deviation measures average spread of data from mean. Small SD: values close to mean; large SD: more variability.' },

{ id: 'q21', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the difference between discrete and continuous probability distributions?',
  a: 'Discrete assigns probabilities to countable outcomes (Binomial, Poisson). Continuous assigns density, probability via area under curve (Normal, Exponential).' },

{ id: 'q22', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is a confidence interval?',
  a: 'A CI provides a plausible range of values for a population parameter, with confidence level (e.g., 95%). E.g., CI=estimate ± margin of error.' },

{ id: 'q23', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'If X~Poisson(λ=3), what is P(X=0)?',
  a: 'P(X=0) = e^(−3)·3^0/0! = e^(−3) ≈ 0.0498.' },

{ id: 'q24', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is an outlier? How can you detect it?',
  a: 'An outlier is an observation far from others. Detection: boxplot rule (1.5×IQR), z-score>3, robust methods.' },

{ id: 'q25', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'Explain the concept of independence of events.',
  a: 'Events A and B are independent if P(A∩B)=P(A)P(B). Knowledge of one does not affect probability of other.' },

{ id: 'q26', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is heteroscedasticity in regression?',
  a: 'Heteroscedasticity = non-constant variance of errors. It violates OLS assumptions, inflates SEs. Detection: residual plots, Breusch-Pagan test. Fix: log-transform, robust SEs.' },

{ id: 'q27', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'What is the probability that in 10 coin tosses exactly 5 heads occur?',
  a: 'Binomial: P= C(10,5)(0.5)^10 = 252/1024 ≈ 0.246.' },

{ id: 'q28', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'Difference between parametric and nonparametric confidence intervals?',
  a: 'Parametric CI relies on distribution assumptions (z/t-based). Nonparametric (bootstrap CI) resamples data without assumptions.' },

{ id: 'q29', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the variance of a fair die roll?',
  a: 'E[X]=3.5; E[X²]=(1²+…+6²)/6=91/6≈15.17. Var=E[X²]–E[X]²=15.17–12.25=2.92.' },

{ id: 'q30', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is statistical power?',
  a: 'Power = P(reject H₀|H₁ true). It measures ability to detect true effect. Increases with sample size, effect size, α.' },

{ id: 'q31', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the expected number of sixes in 60 rolls of a fair die?',
  a: 'Binomial: n=60, p=1/6. Expectation = np = 60·(1/6)=10.' },

{ id: 'q32', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'Difference between parametric mean imputation and multiple imputation?',
  a: 'Mean imputation replaces missing with mean, underestimates variance. Multiple imputation generates multiple plausible values, better uncertainty estimates.' },

{ id: 'q33', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'If cards are drawn from a standard deck, what is the probability the first is an ace and the second is a king (without replacement)?',
  a: 'P= (4/52)(4/51)=16/2652 ≈ 0.00604.' },

{ id: 'q34', topic: 'Statistics', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the median and why is it robust?',
  a: 'Median is middle value of ordered data. Robust because less sensitive to outliers/skewed data than mean.' },

{ id: 'q35', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'If X~N(0,1), what is P(X>1.96)?',
  a: 'From z-tables, P(Z>1.96)=0.025 (one-sided tail).' },

{ id: 'q36', topic: 'Statistics', difficulty: 'Hard', time: '3–5 min',
  q: 'Explain maximum likelihood estimation (MLE).',
  a: 'MLE chooses parameter values that maximize likelihood of observed data. It yields asymptotically unbiased, efficient estimators. Example: Bernoulli with k successes in n trials → p̂=k/n.' },

{ id: 'q37', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the memoryless property of exponential distribution?',
  a: 'For X~Exp(λ): P(X>s+t|X>s)=P(X>t). Future waiting time independent of past. Unique to exponential.' },

{ id: 'q38', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'Difference between ANOVA and regression?',
  a: 'ANOVA compares means across groups; regression models relationship between dependent and independent variables. Mathematically, both use linear models.' },

{ id: 'q39', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is covariance and how is it different from correlation?',
  a: 'Covariance measures joint variability of two variables. Correlation standardizes covariance to (−1,1), unit-free, comparable across scales.' },

{ id: 'q40', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is bootstrapping in statistics?',
  a: 'Bootstrap resamples data with replacement to estimate sampling distribution of a statistic. Useful for CIs, SEs, when parametric assumptions fail.' },

{ id: 'q41', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the distribution of the sum of two independent normals?',
  a: 'If X~N(μ₁,σ₁²), Y~N(μ₂,σ₂²), then X+Y ~ N(μ₁+μ₂, σ₁²+σ₂²).' },

{ id: 'q42', topic: 'Statistics', difficulty: 'Hard', time: '3–5 min',
  q: 'What is the difference between fixed effects and random effects models?',
  a: 'Fixed effects assume entity-specific intercepts (control for time-invariant characteristics). Random effects assume random variation across entities, uncorrelated with regressors.' },

{ id: 'q43', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is a moment generating function (MGF)?',
  a: 'MGF: M_X(t)=E[e^{tX}]. Derivatives at t=0 give moments. MGFs characterize distributions uniquely if they exist.' },

{ id: 'q44', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is logistic regression and when do you use it?',
  a: 'Logistic regression models log-odds of binary outcome as linear in predictors. Used when Y is categorical (yes/no).' },

{ id: 'q45', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'A fair coin is tossed 5 times. What is the probability of at least 4 heads?',
  a: 'P= P(4)+P(5) = C(5,4)/32 + C(5,5)/32 = 5/32+1/32=6/32=0.1875.' },

{ id: 'q46', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is survival analysis?',
  a: 'Survival analysis studies time-to-event data (e.g., failure time, death). Key tools: Kaplan–Meier estimator, Cox proportional hazards model.' },

{ id: 'q47', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the expected value of the minimum of two independent Uniform(0,1) variables?',
  a: 'F_min(x)=1–(1–x)^2, f_min(x)=2(1–x). E[min]=∫0^1 x·2(1–x)dx=1/3.' },

{ id: 'q48', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is Simpson’s paradox?',
  a: 'Simpson’s paradox occurs when a trend in subgroups reverses when data combined, often due to confounding variables.' },

{ id: 'q49', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'For a random variable X~Exponential(λ), find its mean and variance.',
  a: 'E[X]=1/λ, Var[X]=1/λ².' },

{ id: 'q50', topic: 'Statistics', difficulty: 'Hard', time: '3–5 min',
  q: 'Explain principal component analysis (PCA).',
  a: 'PCA transforms correlated variables into uncorrelated components maximizing variance. It reduces dimensionality while retaining most information.' },

{ id: 'q51', topic: 'Probability', difficulty: 'Medium', time: '2–3 min',
  q: 'What is Markov’s inequality?',
  a: 'For nonnegative X and a>0: P(X≥a) ≤ E[X]/a. It bounds tail probability with expectation.' },

{ id: 'q52', topic: 'Statistics', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the difference between cross-sectional and time-series data?',
  a: 'Cross-sectional: multiple subjects at one point. Time-series: one subject over time. Panel: multiple subjects over time.' },

{ id: 'q53', topic: 'Probability', difficulty: 'Hard', time: '3–5 min',
  q: 'A random variable X has pdf f(x)=2x, 0<x<1. Find its mean.',
  a: 'E[X]=∫0^1 x·2x dx=∫0^1 2x^2 dx=2/3.' }

      // --- Machine Learning ---
      { id: 'q4', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
        q: 'Compare L1 and L2 regularization. When would you prefer each?',
        a: 'L1 (Lasso) adds |w| penalty → sparsity, feature selection, robust to outliers; L2 (Ridge) adds w² penalty → shrinks weights, handles multicollinearity, better when many weak features. Elastic Net mixes both.' },
      { id: 'q5', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
        q: 'How do you detect and handle data leakage?',
        a: 'Leakage occurs when features include information unavailable at prediction time (e.g., future timestamps, target-encoding before split). Detect with time-aware splits, auditing feature lineage, model-permutation/SHAP sanity checks, and performance drop in forward-chaining CV. Prevent by strict train/val separation, pipelines that fit only on train, and freezing targets/encodings post split.' },
      { id: 'q6', topic: 'Machine Learning', difficulty: 'Medium', time: '2–4 min',
        q: 'Explain precision, recall, F1, ROC-AUC, and PR-AUC. When prefer PR-AUC?',
        a: 'Precision = TP/(TP+FP), recall = TP/(TP+FN), F1 = harmonic mean. ROC-AUC compares TPR vs FPR; insensitive to prevalence. PR-AUC is better for heavy class imbalance because it focuses on precision vs recall among positives.' },
// --- Machine Learning ---
{ id: 'q7', topic: 'Machine Learning', difficulty: 'Easy', time: '1–2 min',
  q: 'What is the difference between supervised, unsupervised, and reinforcement learning?',
  a: 'Supervised learns from labeled data (classification, regression). Unsupervised learns patterns from unlabeled data (clustering, dimensionality reduction). Reinforcement learning optimizes sequential decisions via reward feedback.' },

{ id: 'q8', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'Why is cross-validation used? How does k-fold differ from leave-one-out?',
  a: 'Cross-validation estimates generalization. k-fold splits into k parts (train on k-1, validate on 1, rotate). Leave-one-out is k=n folds, high variance but low bias, computationally expensive.' },

{ id: 'q9', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'How does gradient boosting differ from bagging?',
  a: 'Bagging builds independent models in parallel (random forests) reducing variance. Boosting builds sequential models correcting previous errors (AdaBoost, XGBoost) reducing bias. Gradient boosting uses gradient descent to minimize loss.' },

{ id: 'q10', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'Explain overfitting. How can it be reduced?',
  a: 'Overfitting = model fits noise, poor generalization. Remedies: regularization (L1/L2), cross-validation, early stopping, dropout, pruning, more data, simpler models.' },

{ id: 'q11', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the difference between bagging and random forests?',
  a: 'Bagging uses bootstrap sampling with aggregation. Random forest adds feature subsampling at each split, decorrelating trees and improving variance reduction.' },

{ id: 'q12', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'What are the assumptions behind linear regression?',
  a: 'Linearity, independence, homoscedasticity, normality of residuals, no perfect multicollinearity. Violations → biased/inefficient estimates.' },

{ id: 'q13', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How do decision trees decide where to split?',
  a: 'They choose splits that maximize impurity reduction using metrics like Gini impurity, entropy (information gain), or variance reduction.' },

{ id: 'q14', topic: 'Machine Learning', difficulty: 'Easy', time: '1–2 min',
  q: 'What is bias–variance tradeoff?',
  a: 'Bias = error from underfitting; variance = error from overfitting. Goal = minimize total error via model complexity tuning.' },

{ id: 'q15', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What are support vector machines (SVMs)?',
  a: 'SVMs find the hyperplane that maximizes margin between classes. Can use kernels to handle non-linear boundaries.' },

{ id: 'q16', topic: 'Machine Learning', difficulty: 'Hard', time: '4–5 min',
  q: 'Explain kernel trick in SVMs.',
  a: 'Kernel trick computes dot products in higher-dimensional feature space without explicit mapping. Allows linear separation in transformed space (RBF, polynomial kernels).' },

{ id: 'q17', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is PCA and why is it used?',
  a: 'Principal Component Analysis projects data onto orthogonal directions maximizing variance. Used for dimensionality reduction, noise filtering, visualization.' },

{ id: 'q18', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How does k-means clustering work?',
  a: 'Initialize k centroids, assign points to nearest centroid, update centroids as means, repeat until convergence. Sensitive to initialization and scale.' },

{ id: 'q19', topic: 'Machine Learning', difficulty: 'Hard', time: '4–5 min',
  q: 'Why is k-means not guaranteed to find the global optimum?',
  a: 'It converges to local minima depending on initialization. Multiple runs (k-means++) help.' },

{ id: 'q20', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'Explain curse of dimensionality in ML.',
  a: 'As dimensions increase, data becomes sparse, distance metrics lose meaning, overfitting risk grows. Remedies: feature selection, dimensionality reduction, regularization.' },

{ id: 'q21', topic: 'Machine Learning', difficulty: 'Easy', time: '1–2 min',
  q: 'What is one-hot encoding? When is it used?',
  a: 'One-hot encoding converts categorical variable with k levels into k binary features. Used in models that can’t handle categories natively (linear models, neural nets).' },

{ id: 'q22', topic: 'Machine Learning', difficulty: 'Hard', time: '4–5 min',
  q: 'Why might accuracy be misleading in imbalanced datasets?',
  a: 'A trivial model predicting the majority class yields high accuracy but fails minority. Better metrics: precision, recall, F1, PR-AUC.' },

{ id: 'q23', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'How does XGBoost improve over traditional gradient boosting?',
  a: 'XGBoost adds regularization, efficient handling of sparse data, parallelization, tree pruning, and advanced stopping criteria, making it faster and less overfit.' },

{ id: 'q24', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is early stopping in neural networks?',
  a: 'Stop training when validation loss stops improving, preventing overfitting by halting before memorization.' },

{ id: 'q25', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is dropout in neural networks?',
  a: 'Dropout randomly disables neurons during training to prevent co-adaptation, improve generalization, acts like model averaging.' },

{ id: 'q26', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'Explain vanishing and exploding gradients in deep networks.',
  a: 'Repeated multiplication in backprop leads to gradients shrinking (vanishing) or growing (exploding). Solutions: ReLU, careful initialization, normalization, residual connections.' },

{ id: 'q27', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is batch normalization?',
  a: 'Batch norm normalizes activations per batch (zero mean, unit variance), stabilizing gradients, speeding convergence, reducing sensitivity to initialization.' },

{ id: 'q28', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How do CNNs differ from traditional feedforward networks?',
  a: 'CNNs use convolutional filters to capture spatial features, weight sharing reduces parameters, pooling provides translation invariance.' },

{ id: 'q29', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is transfer learning?',
  a: 'Transfer learning reuses pretrained model weights on new tasks, fine-tuning with smaller datasets, effective in vision/NLP.' },

{ id: 'q30', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is reinforcement learning’s exploration vs exploitation tradeoff?',
  a: 'Exploration tries new actions to discover rewards; exploitation chooses known best actions. Balance needed for optimal learning.' },

{ id: 'q31', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'How does Q-learning work?',
  a: 'Q-learning updates value of state–action pairs using Bellman equation: Q(s,a)=Q(s,a)+α[r+γmax_a′ Q(s′,a′)−Q(s,a)]. Learns optimal policy via temporal-difference updates.' },

{ id: 'q32', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'What is word2vec? How does skip-gram differ from CBOW?',
  a: 'Word2vec learns embeddings. Skip-gram predicts context from word, CBOW predicts word from context. Skip-gram better for rare words.' },

{ id: 'q33', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'Explain attention mechanism in NLP.',
  a: 'Attention computes weighted sum of encoder hidden states, focusing on relevant context for each output. Improves sequence-to-sequence models, foundation for Transformers.' },

{ id: 'q34', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'What is the Transformer architecture and why did it replace RNNs?',
  a: 'Transformers rely on self-attention instead of recurrence, allowing parallel computation and capturing long dependencies. Outperform RNNs in scalability and accuracy.' },

{ id: 'q35', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'What is overfitting in decision trees? How can pruning help?',
  a: 'Deep trees memorize noise. Pruning removes branches with little predictive power, balancing bias–variance, improving generalization.' },

{ id: 'q36', topic: 'Machine Learning', difficulty: 'Easy', time: '1–2 min',
  q: 'What is k-nearest neighbors (KNN)?',
  a: 'KNN classifies based on majority of k closest neighbors. Nonparametric, sensitive to distance metric and feature scaling.' },

{ id: 'q37', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'What is ensemble learning? Why does it work?',
  a: 'Ensemble learning combines weak learners (bagging, boosting, stacking). Works because averaging reduces variance, sequential correction reduces bias.' },

{ id: 'q38', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'Explain stacking in ensembles.',
  a: 'Stacking trains a meta-learner on outputs of base learners, learning optimal combination. Requires careful CV to avoid leakage.' },

{ id: 'q39', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How is dimensionality reduction different from feature selection?',
  a: 'Feature selection keeps subset of original variables. Dimensionality reduction creates new features (PCA, autoencoders). Both fight curse of dimensionality.' },

{ id: 'q40', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'What is the role of learning rate in gradient descent?',
  a: 'Learning rate controls step size. Too small → slow convergence. Too large → divergence/oscillation. Adaptive methods (Adam, RMSProp) tune automatically.' },

{ id: 'q41', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How does stochastic gradient descent differ from batch gradient descent?',
  a: 'SGD updates weights per sample (noisy but fast). Batch GD updates after full dataset (stable but slow). Mini-batch is compromise.' },

{ id: 'q42', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is regularization in ML?',
  a: 'Regularization adds penalty to loss (L1, L2, dropout) to discourage overly complex models, reducing overfitting.' },

{ id: 'q43', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'Explain variational autoencoders (VAEs).',
  a: 'VAEs are generative models learning latent variable distribution using reparameterization trick. Optimize ELBO combining reconstruction and KL divergence.' },

{ id: 'q44', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'What is GAN mode collapse? How to mitigate?',
  a: 'Mode collapse = generator produces limited variety. Remedies: minibatch discrimination, feature matching, Wasserstein GAN with gradient penalty.' },

{ id: 'q45', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is data augmentation in deep learning?',
  a: 'Artificially increase training data by transformations (rotate, flip, crop). Improves generalization in vision/NLP.' },

{ id: 'q46', topic: 'Machine Learning', difficulty: 'Medium', time: '3–4 min',
  q: 'How to handle missing values in ML pipelines?',
  a: 'Options: drop, mean/median impute, kNN impute, predictive models, indicator for missingness. Always fit imputer on training only.' },

{ id: 'q47', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'What is the bias introduced by target encoding? How to prevent leakage?',
  a: 'Target encoding uses mean of target for category. If fit before splitting, leaks info. Solution: encode within CV folds or use smoothing.' },

{ id: 'q48', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is feature scaling? Why important?',
  a: 'Scaling standardizes feature magnitudes (z-score, min-max). Important for distance-based models (KNN, SVM), gradient descent convergence.' },

{ id: 'q49', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is the elbow method in clustering?',
  a: 'Plot within-cluster sum of squares vs k. The “elbow” indicates optimal k balancing fit and complexity.' },

{ id: 'q50', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'What is concept drift? How do you handle it?',
  a: 'Concept drift = data distribution changes over time, degrading model. Detect with monitoring, handle via retraining, online learning, adaptive models.' },

{ id: 'q51', topic: 'Machine Learning', difficulty: 'Hard', time: '4–5 min',
  q: 'Explain ROC vs PR curves in depth.',
  a: 'ROC plots TPR vs FPR, insensitive to imbalance. PR plots precision vs recall, more informative for rare positives. Prefer PR-AUC in skewed data.' },

{ id: 'q52', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'What is causal inference in ML? How is it different from correlation-based prediction?',
  a: 'Causal inference estimates effect of interventions (e.g., ATE) requiring assumptions (ignorability, unconfoundedness). Prediction models only capture associations.' },

{ id: 'q53', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'Explain SHAP values for model interpretability.',
  a: 'SHAP values allocate feature contributions fairly based on Shapley values from cooperative game theory. Additive, consistent, model-agnostic.' },

{ id: 'q54', topic: 'Machine Learning', difficulty: 'Medium', time: '2–3 min',
  q: 'How does LIME differ from SHAP?',
  a: 'LIME approximates model locally with interpretable surrogate. SHAP uses game theory for consistent global/local explanations. SHAP more principled but heavier.' },

{ id: 'q55', topic: 'Machine Learning', difficulty: 'Hard', time: '4–6 min',
  q: 'What is meta-learning (learning to learn)?',
  a: 'Meta-learning optimizes model training process across tasks. Few-shot learning adapts quickly with small data, methods include MAML, prototypical networks.' },

{ id: 'q56', topic: 'Machine Learning', difficulty: 'Hard', time: '5–6 min',
  q: 'What is reinforcement learning with policy gradients?',
  a: 'Policy gradient directly optimizes πθ(a|s) by ∇θ E[R]. Methods (REINFORCE, Actor–Critic) update parameters toward actions yielding high expected reward.' }

      // --- Deep Learning ---
      { id: 'q7', topic: 'Deep Learning', difficulty: 'Medium', time: '3–4 min',
        q: 'What is batch normalization and why can it help training?',
        a: 'It normalizes layer activations to zero mean/unit variance per mini-batch, then learns scale/shift. It stabilizes gradients, allows higher learning rates, and acts as regularizer. During inference use running averages.' },
      { id: 'q8', topic: 'Deep Learning', difficulty: 'Hard', time: '5–7 min',
        q: 'Describe attention mechanisms and why they outperform RNNs on many sequence tasks.',
        a: 'Attention computes context as a weighted sum of values using query–key similarity, enabling parallelism and long-range dependencies without vanishing gradients. Transformers stack self-attention + feed-forward layers; they scale better and capture global context.' },

      // --- Python & SQL ---
      { id: 'q9', topic: 'Python', difficulty: 'Easy', time: '1–2 min',
        q: 'What is the difference between a list, tuple, and generator in Python?',
        a: 'List: mutable sequence in memory; tuple: immutable, hashable if items hashable; generator: lazy iterator producing items on demand, constant memory.' },
      { id: 'q10', topic: 'SQL', difficulty: 'Medium', time: '2–3 min',
        q: 'Explain LEFT JOIN vs INNER JOIN. When might LEFT JOIN change row counts?',
        a: 'INNER JOIN keeps only matches; LEFT JOIN preserves all left rows with NULLs for non-matches. Row counts can increase with one-to-many relationships unless deduplicated or aggregated.' },
      { id: 'q11', topic: 'SQL', difficulty: 'Hard', time: '4–6 min',
        q: 'How do you detect slow queries and optimize them?',
        a: 'Use EXPLAIN/EXPLAIN ANALYZE, check indexes/selectivity, avoid functions on indexed columns, reduce row scans via predicates, pushdowns, and covering indexes; rewrite subqueries as joins or window functions when appropriate; profile cache/I/O; consider partitioning and materialized views.' },

      // --- Experimentation & Causality ---
      { id: 'q12', topic: 'A/B Testing', difficulty: 'Medium', time: '3–5 min',
        q: 'What is power in hypothesis testing and how do you increase it for an A/B test?',
        a: 'Power = 1 − β, probability to detect a true effect. Increase by larger sample size, lower variance (better metrics/stratification), larger effect, higher α (with caution), or using sequential/Bayesian designs.' },
      { id: 'q13', topic: 'Causality', difficulty: 'Hard', time: '5–7 min',
        q: 'How would you estimate causal impact from observational data?',
        a: 'Use identification strategies: backdoor adjustment with DAGs, matching/propensity scores, inverse probability weighting, difference-in-differences, synthetic control, instrumental variables. Validate with balance checks and sensitivity analyses.' },

      // --- MLOps ---
      { id: 'q14', topic: 'MLOps', difficulty: 'Medium', time: '3–5 min',
        q: 'Outline a robust ML pipeline from data to deployment.',
        a: 'Steps: data versioning; feature engineering with reproducible pipelines; train/val/test split; hyperparameter tuning; model registry; CI for tests; containerization; offline/online eval; shadow or canary deploy; monitoring for performance/data drift; rollback strategy.' },
      { id: 'q15', topic: 'MLOps', difficulty: 'Medium', time: '2–4 min',
        q: 'What is concept drift, and how do you monitor and remediate it?',
        a: 'Concept drift: change in P(y|x). Monitor with performance metrics over time, PSI/JS/Wasserstein on posteriors, and drift detectors. Remediate with re-training, online learning, feature recalibration, or model switchovers.' },

      // --- Visualization & Product ---
      { id: 'q16', topic: 'Visualization', difficulty: 'Easy', time: '1–2 min',
        q: 'When is a heatmap preferable to a scatter plot?',
        a: 'For dense 2D distributions with heavy overplotting; heatmaps (or hex bins) reveal structure by aggregating counts or values per cell.' },
      { id: 'q17', topic: 'Product Sense', difficulty: 'Medium', time: '3–4 min',
        q: 'Design a metric to measure the success of a search ranking change.',
        a: 'Define north-star (e.g., success rate or revenue per search), guardrails (latency, abuse, fairness), user-centered metrics (CTR@k, SAT clicks), and run an A/B test with segment analysis and long-term holdouts.' },

      // --- Case/Practical ---
      { id: 'q18', topic: 'Case Study', difficulty: 'Hard', time: '6–10 min',
        q: 'Your churn model degrades after a pricing change. How do you respond?',
        a: 'Audit data schema and label definitions post-change; check drift on key features/targets; rebaseline with recent cohort; update features sensitive to price (discounts, tenure); compare calibration; consider segmented models; run counterfactual analyses to separate behavior vs pricing effects; roll out with canary and watch guardrails.' },

      // --- Ethics ---
      { id: 'q19', topic: 'Ethics', difficulty: 'Medium', time: '2–3 min',
        q: 'How do you assess fairness in a binary classifier?',
        a: 'Choose fairness notion per context: demographic parity, equalized odds/opportunity, calibration. Compute per-group metrics; analyze trade-offs and perform bias mitigation (reweighing, thresholding, representation learning) with stakeholder input and impact assessment.' },

      // --- Bonus ---
      { id: 'q20', topic: 'Algorithms', difficulty: 'Medium', time: '2–4 min',
        q: 'Explain time vs space complexity trade-offs in feature engineering.',
        a: 'Examples: hashing trick reduces memory vs explicit one-hot; precomputing aggregates speeds training but increases storage; approximate methods (Count-Min Sketch) trade exactness for O(1) updates and sub-linear memory.' },
    ];

    // --- State ---
    const state = {
      search: '',
      difficulty: 'all',
      topics: new Set(),
      sort: 'default',
      onlyBookmarks: false,
      bookmarks: new Set(JSON.parse(localStorage.getItem('dsih_bookmarks')||'[]')),
      theme: localStorage.getItem('dsih_theme') || (matchMedia('(prefers-color-scheme: light)').matches ? 'light' : 'dark'),
    };
    document.documentElement.dataset.theme = state.theme;

    // --- Render topic chips ---
    const topics = [...new Set(QUESTIONS.map(q=>q.topic))].sort();
    const topicChips = el('#topicChips');
    topics.forEach(t => {
      const chip = document.createElement('label');
      chip.className = 'chip';
      chip.innerHTML = `<input type="checkbox" value="${t}"> ${t}`;
      chip.querySelector('input').addEventListener('change', (e)=>{
        if(e.target.checked) state.topics.add(t); else state.topics.delete(t);
        render();
      });
      topicChips.appendChild(chip);
    });

    // --- Helpers ---
    const difficultyRank = { Easy: 1, Medium: 2, Hard: 3 };

    function filtered() {
      let list = QUESTIONS.filter(item => {
        const matchTopic = state.topics.size ? state.topics.has(item.topic) : true;
        const matchDiff = state.difficulty === 'all' ? true : item.difficulty === state.difficulty;
        const matchBookmarks = state.onlyBookmarks ? state.bookmarks.has(item.id) : true;
        let matchSearch = true;
        if (state.search) {
          try {
            const rx = new RegExp(state.search, 'i');
            matchSearch = rx.test(item.q) || rx.test(item.a) || rx.test(item.topic);
          } catch (e) {
            matchSearch = (item.q + item.a + item.topic).toLowerCase().includes(state.search.toLowerCase());
          }
        }
        return matchTopic && matchDiff && matchBookmarks && matchSearch;
      });

      switch(state.sort){
        case 'difficulty': list.sort((a,b)=> difficultyRank[a.difficulty]-difficultyRank[b.difficulty]); break;
        case 'topic': list.sort((a,b)=> a.topic.localeCompare(b.topic)); break;
        case 'alpha': list.sort((a,b)=> a.q.localeCompare(b.q)); break;
        default: break;
      }
      return list;
    }

    function render(){
      const list = filtered();
      el('#count').textContent = `${list.length} results · ${state.topics.size ? [...state.topics].join(', ') : 'All topics'} · ${state.difficulty} difficulty`;
      const grid = el('#grid');
      grid.innerHTML = '';
      list.forEach(item => grid.appendChild(cardFor(item)));
    }

    function cardFor(item){
      const tpl = el('#cardTemplate');
      const node = tpl.content.firstElementChild.cloneNode(true);
      node.dataset.id = item.id;
      el('.topic', node).textContent = item.topic;
      el('.difficulty', node).textContent = item.difficulty;
      el('.time', node).textContent = item.time;
      el('.q', node).textContent = item.q;
      el('.a', node).textContent = item.a;

      const select = el('.selectPrint', node);
      select.addEventListener('change', ()=> node.classList.toggle('bookmark', select.checked));

      const copyBtn = el('.copyBtn', node);
      copyBtn.addEventListener('click', async ()=>{
        const text = `Q: ${item.q}\nA: ${item.a}`;
        try { await navigator.clipboard.writeText(text); copyBtn.textContent = 'Copied!'; setTimeout(()=> copyBtn.textContent = 'Copy', 1200);} catch {}
      });

      const bmBtn = el('.bookmarkBtn', node);
      const updateBm = () => {
        if(state.bookmarks.has(item.id)) { bmBtn.textContent = '★ Saved'; node.classList.add('bookmark'); }
        else { bmBtn.textContent = '☆ Save'; node.classList.remove('bookmark'); }
      };
      updateBm();
      bmBtn.addEventListener('click', ()=>{
        if(state.bookmarks.has(item.id)) state.bookmarks.delete(item.id); else state.bookmarks.add(item.id);
        localStorage.setItem('dsih_bookmarks', JSON.stringify([...state.bookmarks]));
        updateBm();
      });

      return node;
    }

    // --- Events ---
    el('#search').addEventListener('input', (e)=>{ state.search = e.target.value.trim(); render(); });
    el('#clearSearch').addEventListener('click', ()=>{ el('#search').value=''; state.search=''; render(); });
    el('#difficulty').addEventListener('change', (e)=>{ state.difficulty = e.target.value; render(); });
    el('#sort').addEventListener('change', (e)=>{ state.sort = e.target.value; render(); });
    el('#onlyBookmarks').addEventListener('change', (e)=>{ state.onlyBookmarks = e.target.checked; render(); });

    el('#shuffleBtn').addEventListener('click', ()=>{ QUESTIONS.sort(()=>Math.random()-.5); render(); });
    el('#resetBtn').addEventListener('click', ()=>{
      state.search=''; el('#search').value='';
      state.difficulty='all'; el('#difficulty').value='all';
      state.sort='default'; el('#sort').value='default';
      state.topics.clear(); els('#topicChips input').forEach(i=> i.checked=false);
      state.onlyBookmarks=false; el('#onlyBookmarks').checked=false;
      render();
    });

    el('#themeBtn').addEventListener('click', ()=> toggleTheme());
    function toggleTheme(){
      state.theme = (state.theme === 'dark') ? 'light' : 'dark';
      document.documentElement.dataset.theme = state.theme;
      localStorage.setItem('dsih_theme', state.theme);
    }

    // Keyboard shortcuts
    window.addEventListener('keydown', (e)=>{
      if(e.key === 'l' || e.key === 'L') toggleTheme();
      if(e.key === 's' || e.key === 'S') el('#shuffleBtn').click();
      if(e.key === 'p' || e.key === 'P') el('#printBtn').click();
      if(e.key === '/') { e.preventDefault(); el('#search').focus(); }
    });

    // Print & export
    el('#printBtn').addEventListener('click', ()=>{
      const selected = els('.selectPrint:checked').map(cb => cb.closest('.card'));
      if (selected.length === 0) return window.print();
      // Temporarily hide unselected
      const all = els('.card');
      all.forEach(c=> c.classList.add('hidden'));
      selected.forEach(c=> c.classList.remove('hidden'));
      window.print();
      all.forEach(c=> c.classList.remove('hidden'));
    });

    el('#exportBtn').addEventListener('click', ()=>{
      const data = filtered();
      const blob = new Blob([JSON.stringify(data, null, 2)], {type: 'application/json'});
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url; a.download = 'ds-questions.json'; a.click();
      setTimeout(()=> URL.revokeObjectURL(url), 1000);
    });

    // Initial render
    render();
  </script>
</body>
</html>
